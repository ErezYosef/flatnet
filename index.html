<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>FlatNet</title>

    <meta name="description" content="FlatNet">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--FACEBOOK-->
    <meta property="og:image" content="img/twitter-card.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://siddiquesalman.github.io/flatnet/" />
    <meta property="og:title" content="FlatNet" />
    <meta property="og:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="https://siddiquesalman.github.io/flatnet/" />
    <meta name="twitter:title" content="FlatNet" />
    <meta name="twitter:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />
    <meta name="twitter:image" content="https://siddiquesalman.github.io/flatnet/img/twitter-card.jpg" />

    <!-- FAVICONS -->
    <link rel="apple-touch-icon-precomposed" sizes="57x57"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon-precomposed" sizes="114x114"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon-precomposed" sizes="72x72"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon-precomposed" sizes="144x144"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon-precomposed" sizes="60x60"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon-precomposed" sizes="120x120"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon-precomposed" sizes="76x76"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon-precomposed" sizes="152x152"
        href="https://siddiquesalman.github.io/flatnet/favicons/apple-touch-icon-152x152.png" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-196x196.png"
        sizes="196x196" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-96x96.png"
        sizes="96x96" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-32x32.png"
        sizes="32x32" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-16x16.png"
        sizes="16x16" />
    <link rel="icon" type="image/png" href="https://siddiquesalman.github.io/flatnet/favicons/favicon-128.png"
        sizes="128x128" />
    <meta name="application-name" content="FlatNet" />
    <meta name="msapplication-TileColor" content="#FFFFFF" />
    <meta name="msapplication-TileImage"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-144x144.png" />
    <meta name="msapplication-square70x70logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-70x70.png" />
    <meta name="msapplication-square150x150logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-150x150.png" />
    <meta name="msapplication-wide310x150logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-310x150.png" />
    <meta name="msapplication-square310x310logo"
        content="https://siddiquesalman.github.io/flatnet/favicons/mstile-310x310.png" />


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5H2C4DFSMD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-5H2C4DFSMD');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                FlatNet: Towards Photorealistic Scene <br>
                Reconstruction from Lensless Measurements</br>
                <small>
                    IEEE TPAMI 2020
                </small>
            </h2>

        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://siddiquesalman.github.io">
                            Salman S. Khan*
                        </a>
                        </br>IIT Madras
                    </li>
                    <li>
                        <a href="https://varun19299.github.io">
                            Varun Sundar*
                        </a>
                        </br>IIT Madras
                    </li>
                    <li>
                        <a href="https://vivekboominathan.com">
                            Vivek Boominathan
                        </a>
                        </br>Rice University
                    </li>
                    <li>
                        <a href="http://www.ee.iitm.ac.in/kmitra/">
                            Ashok Veeraraghavan
                        </a>
                        </br>Rice University
                    </li>
                    <li>
                        <a href="http://computationalimaging.rice.edu/team/ashok-veeraraghavan/">
                            Kaushik Mitra
                        </a>
                        </br>IIT Madras
                    </li>
                </ul>

                *denotes equal contribution
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-3 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2010.15440">
                            <image src="img/ff_paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/siddiquesalman/flatnet">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a
                            href="https://colab.research.google.com/github/siddiquesalman/flatnet/blob/flatnet-gen/explore_flatnet_gen.ipynb">
                            <image src="img/colab_favicon_256px.png" height="60px">
                                <h4><strong>Collab</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://drive.google.com/drive/folders/1nyng6spi7SQRZb_1zEkOScOIPEI9DCUL?usp=sharing">
                            <image src="img/dataset.png" height="60px">
                                <h4><strong>FlatCam Dataset</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://drive.google.com/drive/folders/1lW9N1jQelxkq2qu6jqB17httgGNpRcvv?usp=sharing">
                            <image src="img/dataset.png" height="60px">
                                <h4><strong>PhlatCam Dataset</strong></h4>
                        </a>
                    </li>

                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/fig_1.jpg" class="img-responsive" alt="overview"><br>
                    <p class="text-justify">
                        Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras
                        by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless
                        cameras rely on computational algorithms to recover the scenes from multiplexed measurements.
                        However, the current iterative-optimization-based reconstruction algorithms produce noisier and
                        perceptually poorer images. In this work, we propose a non-iterative deep learning-based
                        reconstruction approach that
                        results in orders of magnitude improvement in image quality for lensless reconstructions. Our
                        approach, called <b>FlatNet</b>, lays down a framework for reconstructing high-quality
                        photorealistic
                        images from mask-based lensless cameras, where the camera’s forward model formulation is known.
                        <br>
                    </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p class="text-justify">

                <figure>
                    <image src="img/fig_2.jpg" class="img-responsive" onmouseover="this.src='img/fig_2_hover.jpg'"
                        onmouseout="this.src='img/fig_2.jpg'" alt="" />
                    <figcaption><b><br>Method overview.</b> Hover mouse pointer to see details.</figcaption>
                </figure>
                <p class="text-justify">
                    <br /> <b>FlatNet</b> consists of two stages: (1) an inversion stage that maps the measurement
                    into a
                    space of
                    intermediate reconstruction by learning parameters within the forward model formulation, and (2)
                    a perceptual enhancement stage that improves the perceptual quality of this intermediate
                    reconstruction. These stages are trained together in an end-to-end manner. <br>

                    <br>We show high-quality
                    reconstructions by performing extensive experiments on real and challenging scenes using two
                    different types of lensless prototypes: <b><a
                            href=https://intra.ece.ucr.edu/~sasif/papers/2015_AASVR_flatcam_iccv.pdf>FlatCam</a></b>
                    which uses a separable forward model and
                    <b><a href=https://ieeexplore.ieee.org/document/9076617>PhlatCam</a></b>,
                    which uses a more general non-separable cropped-convolution model. Our end-to-end approach is
                    fast, produces photorealistic reconstructions, and is easy to adopt for other mask-based
                    lensless cameras.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Spotlight Video at CVPR 2020
                </h3>

                <br>
                <p align="center">
                <div class="embed-responsive embed-responsive-16by9">
                    <!-- width="500" height="281" -->
                    <iframe src="https://www.youtube.com/embed/GMouWS_Zoa4" frameborder="0"
                        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                </div>

                </p>

                <br> Presented at the CVPR <a href="http://ccd2020.cms.caltech.edu">Computational Cameras and Displays
                    (CCD)</a> Workshop 2020.
                <br>
                <table align=center width=800px>
                    <br>
                    <tr>
                        <center>
                            <span style="font-size:22px">&nbsp;<a
                                    href='https://www.dropbox.com/s/jebzppe1ikdbcld/FlatNet%20CCD%20Spotlight.pptx?dl=0'>[Slides]</a>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    FlatNet without Calibration
                </h3>
                <br />
                <image src="img/uncalib.png" class="img-responsive" alt="overview"><br>

                    <br />Calibration of lensless cameras to obtain the <b>Point Spread Function (PSF)</b> can be a time
                    consuming
                    process and has to be done for each individual camera. Even a small error in calibration can lead to
                    severe degradation in the performance of the reconstruction algorithm.<br />

                    <br />Since <b>FlatNet</b> employs a trainable inversion layer, it does not require careful
                    calibration
                    of the
                    PSF. We provide a specific initialization scheme of the trainable layer for both separable and
                    non-separable cases. Given the mask profile and camera geometry, one can still initialize the
                    inversion layer. Please refer to our <a href="https://arxiv.org/abs/2010.15440">paper</a> for more
                    details.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Working with smaller sensors
                </h3>

                <image src="img/effect_crop.png" class="img-responsive" alt="overview"><br>

                    <br />Another practical scenario that arises in non-separable lensless cameras is the finite size of
                    sensors. Such cameras can be approximated by a convolutional model, and as a consequence, the sensor
                    measurement is the weighted sum of various shifted PSFs. For a large PSF, the measurement can often
                    exceed the
                    sensor size, leading to lost information. <br />

                    <br />While <b>FlatNet</b> is based on the convolutional model, we show how it can be extended to
                    robustly handle smaller sensors with a simple padding scheme. As seen in the figure above, our
                    padding
                    scheme greatly improves the intermediate reconstruction. Finally, our trainable inversion further
                    alleviates visual artefacts, allowing <b>FlatNet</b> to recover scenes on smaller sensors without
                    any
                    significant performance degradation. Please refer to our <a
                        href="https://arxiv.org/abs/2010.15440">paper</a> for more details.

                    <image src="img/effect_learning.png" class="img-responsive" alt="overview"><br>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Lensless Imaging in Indoor Lighting
                </h3>
                <br />
                <image src="img/unconstrained.png" class="img-responsive" alt="overview"><br>


                    <br /> While we trained <b>FlatNet</b> using a monitor-capture scheme, which allows us to
                    inexpensively
                    collect a large dataset, our objective is to recover scenes from real measurements captured in the
                    wild.
                    we finetune FlatNet using a real world dataset we captured called the Unconstrained Indoor
                    Dataset.<br />

                    <br />This dataset consists of unaligned webcam and <b>PhlatCam</b> captures. As seen in the figure
                    above, such a finetuning scheme results in more photorealistic reconstructions.


            </div>

        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <br />
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" rows="14" readonly>
    @ARTICLE {9239993,
    author = {S. Khan and V. Sundar and V. Boominathan and A. Veeraraghavan and K. Mitra},
    journal = {IEEE Transactions on Pattern Analysis & Machine Intelligence},
    title = {FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements},
    year = {2020},
    month = {oct},
    keywords = {cameras;image reconstruction;lenses;multiplexing;computational modeling;mathematical model},
    doi = {10.1109/TPAMI.2020.3033882},
    publisher = {IEEE Computer Society},
    }
    </textarea>
                </div>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related Links
                </h3>
                <p class="text-justify">
                    <br /><b>FlatCam</b> was introduced by <a href="https://arxiv.org/abs/1509.00116">Asif et al.
                        (2015)</a>, using a separable amplitude
                    mask to replace a conventional lens.<br />

                    <br /><a href="https://arxiv.org/abs/1710.02134">Antipa et al. (2017)</a> further showed how off-the
                    sheld diffusers and
                    other random caustics (<b>DiffuserCam</b>) could be used as phase-mask in lensless cameras.<br />

                    <br /><a href="https://ieeexplore.ieee.org/document/9076617">Boominathan et al. (2020)</a> recently
                    developed <b>PhlatCam</b>, which further
                    improves light efficiency and reconstruction quality by using a transparent phase mask.<br />

                    <br />Our previous work, <a
                        href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Khan_Towards_Photorealistic_Reconstruction_of_Highly_Multiplexed_Lensless_Images_ICCV_2019_paper.pdf">Khan
                        et al. (2019)</a> introduced <b>FlatNet</b> for separable
                    lensless models, demonstrating how a learnable inversion layer can be coupled with a image
                    enhancement model such as UNet.<br />

                    <br />Concurrently, <a href="https://arxiv.org/pdf/1908.11502.pdf">Monakhova et al. (2019)</a>
                    proposed <b>Le-ADMM</b> which is a unrolled
                    neural network architecture using learnable ADMM optimization steps.<br />
                    <br />
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <br />This work was supported in part by NSF CAREER: IIS- 1652633, NSF EXPEDITIONS: CCF-1730574,
                    DARPA NESD: HR0011-17-C0026, NIH Grant: R21EY029459 and the Qualcomm Innovation Fellowship India.
                    <br />
                    <br /> This website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and
                    <a href="https://www.matthewtancik.com">Matthew Tannick</a>.
                </p>
            </div>
        </div>
    </div>



    <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Key Contributions
                </h3>
                <br />
                <ul>
                    <li>We propose an efficient implementation for the learnable intermediate stage of a general
                        lensless model. In our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>prior
                            work</a>, we shown this for the separable lensless model. Here
                        we non-trivially extend it to the general lensless case.</li>

                    <figure>
                        <image src="img/fig_7_sim.jpg" class="img-responsive" alt="overview">
                            <figcaption><b>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We verify the robustness of the proposed learnable intermediate mapping for the non-separable
                        lensless model on challenging scenarios where the lensless system does not follow a full
                        convolutional assumption.</li>

                    <li>We propose an initialization scheme for the non- separable lensless model that doesn’t require
                        explicit PSF calibration.</li>

                    <li>Similar to the display and direct captured measurements collected using the separable mask
                        <i><a href=https://intra.ece.ucr.edu/~sasif/papers/2015_AASVR_flatcam_iccv.pdf>FlatCam</a></i>
                        and described in our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>previous
                            work</a>, we collect corresponding datasets for the
                        non-separable mask <i><a href=https://ieeexplore.ieee.org/document/9076617>PhlatCam</a></i>.
                    </li>

                    <br>

                    <figure>
                        <image src="img/dataset3.jpg" class="img-responsive" alt="overview">
                            <figcaption><b><br>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We also collect a dataset of unconstrained indoor lensless measurements paired with
                        corresponding unaligned webcam images which is finally used to finetune our proposed
                        <b>FlatNet</b> to
                        robustly deal with unconstrained real-world scenes.</li>

                    <li>Our method outperforms previous traditional and deep learning based lensless reconstruction
                        methods.</li>
                </ul>
            </div>
        </div> -->




</body>

</html>