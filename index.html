<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>FlatNet</title>

    <meta name="description" content="FlatNet">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->
    <meta property="og:image" content="img/restoration.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://varun19299.github.io/deep-atrous-guided-filter" />
    <meta property="og:title" content="FlatNet" />
    <meta property="og:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />

    <!--TWITTER-->
    <meta name="twitter:card" content="summary" />
    <meta name=”twitter:site” content=”https://varun19299.github.io/deep-atrous-guided-filter/" />
    <meta name="twitter:title" content="Deep Atrous Guided Filter" />
    <meta name="twitter:description"
        content="Project page for FlatNet: Towards Photorealistic Scene Reconstruction from Lensless Measurements." />
    <meta name="twitter:image" content="https://varun19299.github.io/deep-atrous-guided-filter/img/restoration.png" />


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon" type="image/png" href="img/restoration.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-175460063-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-175460063-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                FlatNet: Towards Photorealistic Scene <br>
                Reconstruction from Lensless Measurements</br>
                <!--                 <small>
                    arXiv 2020
                </small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://siddiquesalman.github.io">
                            Salman S. Khan*
                        </a>
                        </br>IIT Madras
                    </li>
                    <li>
                        <a href="https://varun19299.github.io">
                            Varun Sundar*
                        </a>
                        </br>IIT Madras
                    </li>
                    <li>
                        <a href="https://vivekboominathan.com">
                            Vivek Boominathan
                        </a>
                        </br>Rice University
                    </li>
                    <li>
                        <a href="http://www.ee.iitm.ac.in/kmitra/">
                            Ashok Veeraraghavan
                        </a>
                        </br>Rice University
                    </li>
                    <li>
                        <a href="http://computationalimaging.rice.edu/team/ashok-veeraraghavan/">
                            Kaushik Mitra
                        </a>
                        </br>IIT Madras
                    </li>
                    <!-- </br> IIT Madras Rice University -->
                </ul>

                *denotes equal contribution
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                            <image src="img/ff_paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/varun19299/flatnet-gen">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a
                            href="https://colab.research.google.com/github/varun19299/flatnet-gen/blob/master/explore_flatnet_gen.ipynb">
                            <image src="img/colab_favicon_256px.png" height="60px">
                                <h4><strong>Collab</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://drive.google.com/drive/folders/1lW9N1jQelxkq2qu6jqB17httgGNpRcvv?usp=sharing">
                            <image src="img/dataset.png" height="60px">
                                <h4><strong>Dataset</strong></h4>
                        </a>
                    </li>

                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract & Method
                </h3>
                <image src="img/fig_1.jpg" class="img-responsive" alt="overview"><br>
                    <p class="text-justify">
                        Lensless imaging has emerged as a potential solution towards realizing ultra-miniature cameras
                        by eschewing the bulky lens in a traditional camera. Without a focusing lens, the lensless
                        cameras rely on computational algorithms to recover the scenes from multiplexed measurements.
                        However, the current iterative-optimization-based reconstruction algorithms produce noisier and
                        perceptually poorer images. <br />
                    </p>

                    <br />

                    <figure>
                        <image src="img/fig_2.jpg" class="img-responsive" onmouseover="this.src='img/fig_2_hover.jpg'"
                            onmouseout="this.src='img/fig_2.jpg'" alt="" />
                        <figcaption><b><br>Method overview.</b> Hover mouse pointer to see details.</figcaption>
                    </figure>
                    <p class="text-justify">
                        <br /> In this work, we propose a non-iterative deep learning-based reconstruction approach that
                        results in orders of magnitude improvement in image quality for lensless reconstructions. Our
                        approach, called <b>FlatNet</b>, lays down a framework for reconstructing high-quality
                        photorealistic
                        images from mask-based lensless cameras, where the camera’s forward model formulation is known.
                        <br>

                        <br> <b>FlatNet</b> consists of two stages: (1) an inversion stage that maps the measurement
                        into a
                        space of
                        intermediate reconstruction by learning parameters within the forward model formulation, and (2)
                        a perceptual enhancement stage that improves the perceptual quality of this intermediate
                        reconstruction. These stages are trained together in an end-to-end manner. <br>

                        <br>We show high-quality
                        reconstructions by performing extensive experiments on real and challenging scenes using two
                        different types of lensless prototypes: <i><a
                                href=https://intra.ece.ucr.edu/~sasif/papers/2015_AASVR_flatcam_iccv.pdf>FlatCam</a></i>
                        which uses a separable forward model and
                        <i><a href=https://ieeexplore.ieee.org/document/9076617>PhlatCam</a></i>,
                        which uses a more general non-separable cropped-convolution model. Our end-to-end approach is
                        fast, produces photorealistic reconstructions, and is easy to adopt for other mask-based
                        lensless cameras.
                    </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Key Contributions
                </h3>
                <br />
                <ul>
                    <li>We propose an efficient implementation for the learnable intermediate stage of a general
                        lensless model. In our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>prior
                            work</a>, we shown this for the separable lensless model. Here
                        we non-trivially extend it to the general lensless case.</li>

                    <figure>
                        <image src="img/fig_7_sim.jpg" class="img-responsive" alt="overview">
                            <figcaption><b>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We verify the robustness of the proposed learnable intermediate mapping for the non-separable
                        lensless model on challenging scenarios where the lensless system does not follow a full
                        convolutional assumption.</li>

                    <li>We propose an initialization scheme for the non- separable lensless model that doesn’t require
                        explicit PSF calibration.</li>

                    <li>Similar to the display and direct captured measurements collected using the separable mask
                        <i><a href=https://intra.ece.ucr.edu/~sasif/papers/2015_AASVR_flatcam_iccv.pdf>FlatCam</a></i>
                        and described in our <a href=https://siddiquesalman.github.io/flatcam_iccv.html>previous
                            work</a>, we collect corresponding datasets for the
                        non-separable mask <i><a href=https://ieeexplore.ieee.org/document/9076617>PhlatCam</a></i>.
                    </li>

                    <br>

                    <figure>
                        <image src="img/dataset3.jpg" class="img-responsive" alt="overview">
                            <figcaption><b><br>Some simulated outputs.</b> Notice how closely the simulated measurements
                                resemble real ones.
                            </figcaption>
                    </figure>

                    <li>We also collect a dataset of unconstrained indoor lensless measurements paired with
                        corresponding unaligned webcam images which is finally used to finetune our proposed
                        <b>FlatNet</b> to
                        robustly deal with unconstrained real-world scenes.</li>

                    <li>Our method outperforms previous traditional and deep learning based lensless reconstruction
                        methods.</li>
                </ul>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Simulation Procedure
                </h3>
                <figure>
                    <image src="img/fig_7_sim.jpg" class="img-responsive" alt="overview">
                        <figcaption><b>Some simulated outputs.</b> Notice how closely the simulated measurements
                            resemble real ones.
                        </figcaption>
                </figure>
                <p class="text-justify">
                    <br /> Availability of data, even with clever schemes like <a
                        href="https://arxiv.org/abs/2003.04857">monitor acquisition</a>, can be a constraining
                    factor
                    while designing learning based approaches in imaging pipelines. Instead, we propose a simple
                    simulation scheme to cheaply generate training data.<br />

                    <br />We train a shallow version of DAGF to transform clean <a
                        href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">DIV2K images</a> to various display
                    measurements
                    (glass, POLED or TOLED). We use this data to pre-train DAGF, which provides us a performance boost
                    of 0.3 to 0.5 dB in PSNR. See our paper for more details.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>Results </h3>
                <figure>
                    <image src="img/fig_5_poled.jpg" class="img-responsive" alt="overview">
                        <figcaption><b>Qualitative comparison on POLED measurements.</b>
                        </figcaption>
                </figure>
                <p class="text-justify">
                    <br>
                    DAGF's ability to directly train on megapixel images and hence aggregate contextual information
                    over large receptive fields leads to a superior restoration. </br>

                    </br>We show significant improvement over
                    exisitng state-of-the-art
                    image-restoration methods, which are designed for tasks such as deraining, dehazing and image
                    transformation. Such methods lack sufficient input context for a challenging scenario such as
                    UDC.</br>


                    </br>This is more evident on the POLED
                    dataset, where line, colour and blur artefacts can be seen in the baselines.</br>
                </p>

                <figure>
                    <image src="img/fig_6_toled.jpg" class="img-responsive" alt="overview">
                        <figcaption><b>Qualitative comparison on TOLED measurements.</b>
                        </figcaption>
                </figure>

                <p class="text-justify">
                    <br> The baselines perform better on the moderatly degraded TOLED measurements, but DAGF still
                    surpasses them visually and metric-wise.
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Spotlight at CVPR CCD 2020
                </h3>

                <br>
                <p align="center">
                <div class="embed-responsive embed-responsive-16by9">
                    <!-- width="600" height="338" -->
                    <iframe src="https://www.youtube.com/embed/GMouWS_Zoa4" frameborder="0"
                        allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen></iframe>
                </div>

                </p>

                <br> Presented at the CVPR Computational Cameras and Displays (CCD) Workshop 2020.
                <br>
                <table align=center width=800px>
                    <br>
                    <tr>
                        <center>
                            <span style="font-size:22px">&nbsp;<a
                                    href='https://www.dropbox.com/s/jebzppe1ikdbcld/FlatNet%20CCD%20Spotlight.pptx?dl=0'>[Slides]</a>
                </table>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <br />
                <!-- <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" rows="9" readonly>
@misc{sundar2020deep,
title={Deep Atrous Guided Filter for Image Restoration in Under Display Cameras},
author={Varun Sundar and Sumanth Hegde and Divya Kothandaraman and Kaushik Mitra},
year={2020},
eprint={2008.06229},
archivePrefix={arXiv},
primaryClass={cs.CV}
}</textarea>
                </div> -->
                Will be updated soon.

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <br />This work was supported in part by NSF CAREER: IIS- 1652633, NSF EXPEDITIONS: CCF-1730574,
                    DARPA NESD: HR0011-17-C0026, NIH Grant: R21EY029459 and the Qualcomm Innovation Fellowship India.
                    <br />
                    <br /> This website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and
                    <a href="https://www.matthewtancik.com">Matthew Tannick</a>.
                </p>
            </div>
        </div>
    </div>
</body>

</html>